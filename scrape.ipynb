{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import requests\n",
    "import urllib.request\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import Comment\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\krnps\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from webdriver-manager) (2.32.3)\n",
      "Collecting python-dotenv (from webdriver-manager)\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\krnps\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from webdriver-manager) (24.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\krnps\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->webdriver-manager) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\krnps\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->webdriver-manager) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\krnps\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->webdriver-manager) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\krnps\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->webdriver-manager) (2025.1.31)\n",
      "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv, webdriver-manager\n",
      "Successfully installed python-dotenv-1.1.0 webdriver-manager-4.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver-manager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sofa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchDriverException",
     "evalue": "Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:64\u001b[39m, in \u001b[36mDriverFinder._binary_paths\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(path).is_file():\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe path is not a valid file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     65\u001b[39m \u001b[38;5;28mself\u001b[39m._paths[\u001b[33m\"\u001b[39m\u001b[33mdriver_path\u001b[39m\u001b[33m\"\u001b[39m] = path\n",
      "\u001b[31mValueError\u001b[39m: The path is not a valid file: /usr/local/bin/chromedriver",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mNoSuchDriverException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m chrome_driver_path = \u001b[33m\"\u001b[39m\u001b[33m/usr/local/bin/chromedriver\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     13\u001b[39m service = Service(chrome_driver_path)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m driver = \u001b[43mwebdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43mservice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mservice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Define directory\u001b[39;00m\n\u001b[32m     17\u001b[39m data_dir = \u001b[33m'\u001b[39m\u001b[33msofa\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py:45\u001b[39m, in \u001b[36mWebDriver.__init__\u001b[39m\u001b[34m(self, options, service, keep_alive)\u001b[39m\n\u001b[32m     42\u001b[39m service = service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[32m     43\u001b[39m options = options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbrowser_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbrowserName\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvendor_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgoog\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:50\u001b[39m, in \u001b[36mChromiumDriver.__init__\u001b[39m\u001b[34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28mself\u001b[39m.service = service\n\u001b[32m     49\u001b[39m finder = DriverFinder(\u001b[38;5;28mself\u001b[39m.service, options)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfinder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_browser_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     51\u001b[39m     options.binary_location = finder.get_browser_path()\n\u001b[32m     52\u001b[39m     options.browser_version = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:47\u001b[39m, in \u001b[36mDriverFinder.get_browser_path\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_browser_path\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_binary_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mbrowser_path\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:78\u001b[39m, in \u001b[36mDriverFinder._binary_paths\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     77\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnable to obtain driver for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbrowser\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NoSuchDriverException(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._paths\n",
      "\u001b[31mNoSuchDriverException\u001b[39m: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "# Set up ChromeDriver properly\n",
    "chrome_driver_path = \"/usr/local/bin/chromedriver\"\n",
    "service = Service(chrome_driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Define directory\n",
    "data_dir = 'sofa'\n",
    "product_dir = os.path.join('images', data_dir)\n",
    "os.makedirs(product_dir, exist_ok=True)\n",
    "\n",
    "# URL to scrape\n",
    "url = 'https://www.allmodern.com/furniture/cat/sofas-sectionals-c527685.html#sbprodgrid'\n",
    "driver.get(url)\n",
    "\n",
    "# Create dataframe\n",
    "full_df_sofa = pd.DataFrame(columns=['id', 'website_link', 'image_link', 'prices', 'category'])\n",
    "counter = 0  \n",
    "\n",
    "for i in range(1, 11):\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Scroll down to load images\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    img_container = soup.find_all('div', class_='Category-productWrap')\n",
    "\n",
    "    image_links, prices, website_links, ids = [], [], [], []\n",
    "\n",
    "    for img_div in img_container:\n",
    "        # Get image link\n",
    "        image_tag = img_div.find('img', src=True)\n",
    "        if image_tag:\n",
    "            image_url = image_tag['src']\n",
    "            if 'data:image' not in image_url:\n",
    "                image_filename = f\"{product_dir}/{data_dir}_{counter}.jpg\"\n",
    "                urllib.request.urlretrieve(image_url, image_filename)\n",
    "                image_links.append(image_url)\n",
    "                ids.append(f\"{data_dir}_{counter}\")\n",
    "                counter += 1\n",
    "\n",
    "        # Get product link\n",
    "        link_tag = img_div.find('a', href=True)\n",
    "        if link_tag:\n",
    "            website_links.append(link_tag['href'])\n",
    "\n",
    "        # Get price\n",
    "        price_tag = img_div.find('span', class_='ProductCard-price')\n",
    "        if price_tag:\n",
    "            prices.append(price_tag.text.strip())\n",
    "\n",
    "    # Store in DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'id': ids,\n",
    "        'website_link': website_links,\n",
    "        'image_link': image_links,\n",
    "        'prices': prices,\n",
    "        'category': data_dir\n",
    "    })\n",
    "\n",
    "    full_df_sofa = pd.concat([full_df_sofa, data], ignore_index=True)\n",
    "\n",
    "    # Pagination\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, '//*[@id=\"bd\"]/div[2]/div[2]/nav/a[{}]'.format(i))\n",
    "        ActionChains(driver).move_to_element(next_button).click().perform()\n",
    "    except:\n",
    "        print(\"No more pages or navigation failed.\")\n",
    "        break\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'capabilities'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m     os.makedirs(product_dir)\n\u001b[32m      8\u001b[39m path = \u001b[33m\"\u001b[39m\u001b[33m/usr/local/bin/chromedriver\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m driver = \u001b[43mwebdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m url = \u001b[33m'\u001b[39m\u001b[33mhttps://www.allmodern.com/furniture/cat/sofas-sectionals-c527685.html#sbprodgrid\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     12\u001b[39m driver.get(url)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py:45\u001b[39m, in \u001b[36mWebDriver.__init__\u001b[39m\u001b[34m(self, options, service, keep_alive)\u001b[39m\n\u001b[32m     42\u001b[39m service = service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[32m     43\u001b[39m options = options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbrowser_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbrowserName\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvendor_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgoog\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:50\u001b[39m, in \u001b[36mChromiumDriver.__init__\u001b[39m\u001b[34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28mself\u001b[39m.service = service\n\u001b[32m     49\u001b[39m finder = DriverFinder(\u001b[38;5;28mself\u001b[39m.service, options)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfinder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_browser_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     51\u001b[39m     options.binary_location = finder.get_browser_path()\n\u001b[32m     52\u001b[39m     options.browser_version = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:47\u001b[39m, in \u001b[36mDriverFinder.get_browser_path\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_browser_path\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_binary_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mbrowser_path\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:56\u001b[39m, in \u001b[36mDriverFinder._binary_paths\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._paths[\u001b[33m\"\u001b[39m\u001b[33mdriver_path\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._paths\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m browser = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcapabilities\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mbrowserName\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     58\u001b[39m     path = \u001b[38;5;28mself\u001b[39m._service.path\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'capabilities'"
     ]
    }
   ],
   "source": [
    "data_dir = 'sofa'\n",
    "\n",
    "product_dir = os.path.join('images', data_dir)\n",
    "counter = 0\n",
    "if not os.path.exists(product_dir):\n",
    "    os.makedirs(product_dir)\n",
    "\n",
    "path = \"/usr/local/bin/chromedriver\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "url = 'https://www.allmodern.com/furniture/cat/sofas-sectionals-c527685.html#sbprodgrid'\n",
    "driver.get(url)\n",
    "\n",
    "full_df_sofa = pd.DataFrame(columns=['id', 'website_link', 'image_link', 'prices', 'category'])\n",
    "counter = 0 \n",
    "\n",
    "for i in range(1, 11):\n",
    "    time.sleep(2)\n",
    "    #driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    if  5 > i >= 2:\n",
    "        i = i+1\n",
    "        \n",
    "    elif 9 > i >= 5:\n",
    "        i = 4\n",
    "    \n",
    "    elif i == 9:\n",
    "        i = 5\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    img = soup.find('div', {'class': 'Category-productWrap is-loaded'})\n",
    "\n",
    "    image_link = []\n",
    "    prices = []\n",
    "    website_link = []\n",
    "    id = []\n",
    "\n",
    "    for image in img.findAll('img', attrs={'srcset':True}):\n",
    "        image = image.attrs['srcset'][1+image.attrs['srcset'].find(','):image.attrs['srcset'].find('.310w')-4]\n",
    "        if ('data:image') in image:\n",
    "                pass\n",
    "        else:\n",
    "            urllib.request.urlretrieve(image, product_dir+'/'+data_dir+str(counter)+'.jpg')\n",
    "            image_link.append(image)\n",
    "            id.append(data_dir+str(counter))\n",
    "            counter += 1\n",
    "    \n",
    "    for website in img.findAll('a', {'href':True}):\n",
    "        website = website['href']\n",
    "        website_link.append(website)\n",
    "\n",
    "\n",
    "    for price in img.findAll('div',{'class':'ProductCard-pricing'}):\n",
    "        price = price.find('span', {'class':'ProductCard-price'}).text\n",
    "        prices.append(price)\n",
    "\n",
    "    data = pd.DataFrame(dict(zip(['id', 'website_link', 'image_link', 'prices'],[id, website_link, image_link, prices])))\n",
    "    data['category'] = data_dir\n",
    "    \n",
    "    full_df_sofa = full_df_sofa.append(data)\n",
    "\n",
    "    if i == 10:\n",
    "        break\n",
    "    \n",
    "    element = driver.find_element_by_xpath('//*[@id=\"bd\"]/div[2]/div[2]/nav/a[{}]'.format(i))\n",
    "    element.click()\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_chair = pd.DataFrame(columns=['website_link', 'image_link', 'prices', 'category'])\n",
    "\n",
    "data_dir = 'chairs'\n",
    "\n",
    "product_dir = os.path.join('images', data_dir)\n",
    "counter = 0\n",
    "if not os.path.exists(product_dir):\n",
    "    os.makedirs(product_dir)\n",
    "\n",
    "path = \"/usr/local/bin/chromedriver\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "url = 'https://www.allmodern.com/furniture/sb0/accent-chairs-c365818.html'\n",
    "driver.get(url)\n",
    "\n",
    "counter = 0 \n",
    "\n",
    "for i in range(1, 11):\n",
    "    time.sleep(2)\n",
    "    #driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    if  5 > i >= 2:\n",
    "        i = i+1\n",
    "        \n",
    "    elif 9 > i >= 5:\n",
    "        i = 4\n",
    "    \n",
    "    elif i == 9:\n",
    "        i = 5\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    img = soup.find('div', {'data-hb-id': 'pl-grid-v2'})\n",
    "\n",
    "    image_link = []\n",
    "    prices = []\n",
    "    website_link = []\n",
    "\n",
    "    for image in img.findAll('img', attrs={'srcset':True}):\n",
    "        image = image.attrs['srcset'][1+image.attrs['srcset'].find(','):image.attrs['srcset'].find('.310w')-4]\n",
    "        if ('data:image') in image:\n",
    "                pass\n",
    "        else:\n",
    "            urllib.request.urlretrieve(image, product_dir+'/'+data_dir+str(counter)+'.jpg')\n",
    "            image_link.append(image)\n",
    "            counter += 1\n",
    "    \n",
    "    for website in img.findAll('a', {'href':True}):\n",
    "        website = website['href']\n",
    "        website_link.append(website)\n",
    "\n",
    "    if i == 1:\n",
    "        for price in img.findAll('div',{'class':'ProductCard-pricing'}):\n",
    "            price = price.find('span', {'class':'ProductCard-price'}).text\n",
    "            prices.append(price)    \n",
    "    else:\n",
    "        for price in img.findAll('div',{'class':'BrowseMinimizedPriceBlock'}):\n",
    "            price = price.find('span', {'class':'BrowseMinimizedPriceBlock-price'}).text\n",
    "            prices.append(price)\n",
    "\n",
    "\n",
    "    data = pd.DataFrame(dict(zip(['website_link', 'image_link', 'prices'],[website_link, image_link, prices])))\n",
    "    data['category'] = data_dir\n",
    "    full_df_chair = full_df_chair.append(data)\n",
    "\n",
    "    if i == 10:\n",
    "        break\n",
    "\n",
    "    element = driver.find_element_by_xpath('//*[@id=\"sbprodgrid\"]/div[3]/div/nav/a[{}]'.format(i))\n",
    "    element.click()\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Console Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_consoletable = pd.DataFrame(columns=['website_link', 'image_link', 'prices', 'category'])\n",
    "\n",
    "data_dir = 'console table'\n",
    "\n",
    "product_dir = os.path.join('images', data_dir)\n",
    "counter = 0\n",
    "if not os.path.exists(product_dir):\n",
    "    os.makedirs(product_dir)\n",
    "\n",
    "path = \"/usr/local/bin/chromedriver\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "url = 'https://www.allmodern.com/furniture/sb0/console-tables-c413349.html'\n",
    "driver.get(url)\n",
    "\n",
    "counter = 0 \n",
    "\n",
    "for i in range(1, 5):\n",
    "    time.sleep(2)\n",
    "    #driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    if  5 > i >= 2:\n",
    "        i = i+1\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    img = soup.find('div', {'data-hb-id': 'pl-grid-v2'})\n",
    "\n",
    "    image_link = []\n",
    "    prices = []\n",
    "    website_link = []\n",
    "\n",
    "    for image in img.findAll('img', attrs={'srcset':True}):\n",
    "        image = image.attrs['srcset'][1+image.attrs['srcset'].find(','):image.attrs['srcset'].find('.310w')-4]\n",
    "        if ('data:image') in image:\n",
    "                pass\n",
    "        else:\n",
    "            urllib.request.urlretrieve(image, product_dir+'/'+data_dir+str(counter)+'.jpg')\n",
    "            image_link.append(image)\n",
    "            counter += 1\n",
    "    \n",
    "    for website in img.findAll('a', {'href':True}):\n",
    "        website = website['href']\n",
    "        website_link.append(website)\n",
    "\n",
    "    if i == 1:\n",
    "        for price in img.findAll('div',{'class':'ProductCard-pricing'}):\n",
    "            price = price.find('span', {'class':'ProductCard-price'}).text\n",
    "            prices.append(price)    \n",
    "    else:\n",
    "        for price in img.findAll('div',{'class':'BrowseMinimizedPriceBlock'}):\n",
    "            price = price.find('span', {'class':'BrowseMinimizedPriceBlock-price'}).text\n",
    "            prices.append(price)\n",
    "\n",
    "\n",
    "    data = pd.DataFrame(dict(zip(['website_link', 'image_link', 'prices'],[website_link, image_link, prices])))\n",
    "    data['category'] = data_dir\n",
    "    full_df_consoletable = full_df_consoletable.append(data)\n",
    "    \n",
    "    if i == 5:\n",
    "        break\n",
    "    element = driver.find_element_by_xpath('//*[@id=\"sbprodgrid\"]/div[3]/div/nav/a[{}]'.format(i))\n",
    "    element.click()\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TV Stands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_tvstand = pd.DataFrame(columns=['website_link', 'image_link', 'prices', 'category'])\n",
    "\n",
    "data_dir = 'tv stand'\n",
    "\n",
    "product_dir = os.path.join('images', data_dir)\n",
    "counter = 0\n",
    "if not os.path.exists(product_dir):\n",
    "    os.makedirs(product_dir)\n",
    "\n",
    "path = \"/usr/local/bin/chromedriver\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "url = 'https://www.allmodern.com/furniture/sb0/tv-stands-c445006.html'\n",
    "driver.get(url)\n",
    "\n",
    "counter = 0 \n",
    "\n",
    "for i in range(1, 5):\n",
    "    time.sleep(2)\n",
    "    #driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    if  5 > i >= 2:\n",
    "        i = i+1\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    img = soup.find('div', {'data-hb-id': 'pl-grid-v2'})\n",
    "\n",
    "    image_link = []\n",
    "    prices = []\n",
    "    website_link = []\n",
    "\n",
    "    for image in img.findAll('img', attrs={'srcset':True}):\n",
    "        image = image.attrs['srcset'][1+image.attrs['srcset'].find(','):image.attrs['srcset'].find('.310w')-4]\n",
    "        if ('data:image') in image:\n",
    "                pass\n",
    "        else:\n",
    "            urllib.request.urlretrieve(image, product_dir+'/'+data_dir+str(counter)+'.jpg')\n",
    "            image_link.append(image)\n",
    "            counter += 1\n",
    "    \n",
    "    for website in img.findAll('a', {'href':True}):\n",
    "        website = website['href']\n",
    "        website_link.append(website)\n",
    "\n",
    "    if i == 1:\n",
    "        for price in img.findAll('div',{'class':'ProductCard-pricing'}):\n",
    "            price = price.find('span', {'class':'ProductCard-price'}).text\n",
    "            prices.append(price)    \n",
    "    else:\n",
    "        for price in img.findAll('div',{'class':'BrowseMinimizedPriceBlock'}):\n",
    "            price = price.find('span', {'class':'BrowseMinimizedPriceBlock-price'}).text\n",
    "            prices.append(price)\n",
    "\n",
    "\n",
    "    data = pd.DataFrame(dict(zip(['website_link', 'image_link', 'prices'],[website_link, image_link, prices])))\n",
    "    data['category'] = data_dir\n",
    "    full_df_tvstand = full_df_tvstand.append(data)\n",
    "    \n",
    "    if i == 5:\n",
    "        break\n",
    "    element = driver.find_element_by_xpath('//*[@id=\"sbprodgrid\"]/div[3]/div/nav/a[{}]'.format(i))\n",
    "    element.click()\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coffee Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_coffeetable = pd.DataFrame(columns=['website_link', 'image_link', 'prices', 'category'])\n",
    "\n",
    "data_dir = 'coffee tables'\n",
    "\n",
    "product_dir = os.path.join('images', data_dir)\n",
    "counter = 0\n",
    "if not os.path.exists(product_dir):\n",
    "    os.makedirs(product_dir)\n",
    "\n",
    "path = \"/usr/local/bin/chromedriver\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "url = 'https://www.allmodern.com/furniture/sb0/coffee-tables-c413347.html'\n",
    "driver.get(url)\n",
    "\n",
    "counter = 0 \n",
    "\n",
    "for i in range(1, 10):\n",
    "    time.sleep(2)\n",
    "    #driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    if  5 > i >= 2:\n",
    "        i = i+1\n",
    "        \n",
    "    elif 8 > i >= 5:\n",
    "        i = 4\n",
    "    \n",
    "    elif i == 8:\n",
    "        i = 5\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    img = soup.find('div', {'data-hb-id': 'pl-grid-v2'})\n",
    "\n",
    "    image_link = []\n",
    "    prices = []\n",
    "    website_link = []\n",
    "\n",
    "    for image in img.findAll('img', attrs={'srcset':True}):\n",
    "        image = image.attrs['srcset'][1+image.attrs['srcset'].find(','):image.attrs['srcset'].find('.310w')-4]\n",
    "        if ('data:image') in image:\n",
    "                pass\n",
    "        else:\n",
    "            urllib.request.urlretrieve(image, product_dir+'/'+data_dir+str(counter)+'.jpg')\n",
    "            image_link.append(image)\n",
    "            counter += 1\n",
    "    \n",
    "    for website in img.findAll('a', {'href':True}):\n",
    "        website = website['href']\n",
    "        website_link.append(website)\n",
    "\n",
    "    if i == 1:\n",
    "        for price in img.findAll('div',{'class':'ProductCard-pricing'}):\n",
    "            price = price.find('span', {'class':'ProductCard-price'}).text\n",
    "            prices.append(price)    \n",
    "    else:\n",
    "        for price in img.findAll('div',{'class':'BrowseMinimizedPriceBlock'}):\n",
    "            price = price.find('span', {'class':'BrowseMinimizedPriceBlock-price'}).text\n",
    "            prices.append(price)\n",
    "\n",
    "\n",
    "    data = pd.DataFrame(dict(zip(['website_link', 'image_link', 'prices'],[website_link, image_link, prices])))\n",
    "    data['category'] = data_dir\n",
    "    full_df_coffeetable = full_df_coffeetable.append(data)\n",
    "\n",
    "    if i == 9:\n",
    "        break\n",
    "    element = driver.find_element_by_xpath('//*[@id=\"sbprodgrid\"]/div[3]/div/nav/a[{}]'.format(i))\n",
    "    element.click()\n",
    "    \n",
    "    time.sleep(10)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_endtable = pd.DataFrame(columns=['website_link', 'image_link', 'prices', 'category'])\n",
    "\n",
    "data_dir = 'end tables'\n",
    "\n",
    "product_dir = os.path.join('images', data_dir)\n",
    "counter = 0\n",
    "if not os.path.exists(product_dir):\n",
    "    os.makedirs(product_dir)\n",
    "\n",
    "path = \"/usr/local/bin/chromedriver\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "url = 'https://www.allmodern.com/furniture/sb0/end-side-tables-c413348.html'\n",
    "driver.get(url)\n",
    "\n",
    "counter = 0 \n",
    "\n",
    "for i in range(1, 10):\n",
    "    time.sleep(2)\n",
    "    #driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    if  5 > i >= 2:\n",
    "        i = i+1\n",
    "        \n",
    "    elif 8 > i >= 5:\n",
    "        i = 4\n",
    "\n",
    "    elif i == 8:\n",
    "        i = 5\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    img = soup.find('div', {'data-hb-id': 'pl-grid-v2'})\n",
    "\n",
    "    image_link = []\n",
    "    prices = []\n",
    "    website_link = []\n",
    "\n",
    "    for image in img.findAll('img', attrs={'srcset':True}):\n",
    "        image = image.attrs['srcset'][1+image.attrs['srcset'].find(','):image.attrs['srcset'].find('.310w')-4]\n",
    "        if ('data:image') in image:\n",
    "                pass\n",
    "        else:\n",
    "            urllib.request.urlretrieve(image, product_dir+'/'+data_dir+str(counter)+'.jpg')\n",
    "            image_link.append(image)\n",
    "            counter += 1\n",
    "    \n",
    "    for website in img.findAll('a', {'href':True}):\n",
    "        website = website['href']\n",
    "        website_link.append(website)\n",
    "\n",
    "    if i == 1:\n",
    "        for price in img.findAll('div',{'class':'ProductCard-pricing'}):\n",
    "            price = price.find('span', {'class':'ProductCard-price'}).text\n",
    "            prices.append(price)    \n",
    "    else:\n",
    "        for price in img.findAll('div',{'class':'BrowseMinimizedPriceBlock'}):\n",
    "            price = price.find('span', {'class':'BrowseMinimizedPriceBlock-price'}).text\n",
    "            prices.append(price)\n",
    "\n",
    "\n",
    "    data = pd.DataFrame(dict(zip(['website_link', 'image_link', 'prices'],[website_link, image_link, prices])))\n",
    "    data['category'] = data_dir\n",
    "    full_df_endtable = full_df_endtable.append(data)\n",
    "\n",
    "    if i == 9:\n",
    "        break\n",
    "\n",
    "    element = driver.find_element_by_xpath('//*[@id=\"sbprodgrid\"]/div[3]/div/nav/a[{}]'.format(i))\n",
    "    element.click()\n",
    "    \n",
    "    time.sleep(10)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ottomans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_ottomans = pd.DataFrame(columns=['website_link', 'image_link', 'prices', 'category'])\n",
    "\n",
    "data_dir = 'ottomans'\n",
    "\n",
    "product_dir = os.path.join('images', data_dir)\n",
    "counter = 0\n",
    "if not os.path.exists(product_dir):\n",
    "    os.makedirs(product_dir)\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--disable-blink-features=AutomationControlled') \n",
    "path = \"/usr/local/bin/chromedriver\"\n",
    "driver = webdriver.Chrome(path, options=options)\n",
    "\n",
    "url = 'https://www.allmodern.com/furniture/sb0/ottomans-poufs-c365824.html'\n",
    "driver.get(url)\n",
    "\n",
    "counter = 0 \n",
    "\n",
    "for i in range(1, 6):\n",
    "    time.sleep(2)\n",
    "    #driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    if  6 > i >= 2:\n",
    "        i = i+1\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    img = soup.find('div', {'data-hb-id': 'pl-grid-v2'})\n",
    "\n",
    "    image_link = []\n",
    "    prices = []\n",
    "    website_link = []\n",
    "\n",
    "    for image in img.findAll('img', attrs={'srcset':True}):\n",
    "        image = image.attrs['srcset'][1+image.attrs['srcset'].find(','):image.attrs['srcset'].find('.310w')-4]\n",
    "        if ('data:image') in image:\n",
    "                pass\n",
    "        else:\n",
    "            urllib.request.urlretrieve(image, product_dir+'/'+data_dir+str(counter)+'.jpg')\n",
    "            image_link.append(image)\n",
    "            counter += 1\n",
    "    \n",
    "    for website in img.findAll('a', {'href':True}):\n",
    "        website = website['href']\n",
    "        website_link.append(website)\n",
    "\n",
    "    if i == 1:\n",
    "        for price in img.findAll('div',{'class':'ProductCard-pricing'}):\n",
    "            price = price.find('span', {'class':'ProductCard-price'}).text\n",
    "            prices.append(price)    \n",
    "    else:\n",
    "        for price in img.findAll('div',{'class':'BrowseMinimizedPriceBlock'}):\n",
    "            price = price.find('span', {'class':'BrowseMinimizedPriceBlock-price'}).text\n",
    "            prices.append(price)\n",
    "\n",
    "\n",
    "    data = pd.DataFrame(dict(zip(['website_link', 'image_link', 'prices'],[website_link, image_link, prices])))\n",
    "    data['category'] = data_dir\n",
    "    full_df_ottomans = full_df_ottomans.append(data)\n",
    "    \n",
    "    if i == 6:\n",
    "        break\n",
    "    \n",
    "    element = driver.find_element_by_xpath('//*[@id=\"sbprodgrid\"]/div[3]/div/nav/a[{}]'.format(i))\n",
    "    element.click()\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_lamp = pd.DataFrame(columns=['website_link', 'image_link', 'prices', 'category'])\n",
    "\n",
    "data_dir = 'lamp'\n",
    "\n",
    "product_dir = os.path.join('images', data_dir)\n",
    "counter = 0\n",
    "if not os.path.exists(product_dir):\n",
    "    os.makedirs(product_dir)\n",
    "\n",
    "path = \"/usr/local/bin/chromedriver\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "url = 'https://www.allmodern.com/lighting/sb0/table-lamps-c1872230.html'\n",
    "driver.get(url)\n",
    "\n",
    "counter = 0 \n",
    "\n",
    "for i in range(1, 5):\n",
    "    time.sleep(2)\n",
    "    #driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    if  5 > i >= 2:\n",
    "        i = i+1\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    img = soup.find('div', {'data-hb-id': 'pl-grid-v2'})\n",
    "\n",
    "    image_link = []\n",
    "    prices = []\n",
    "    website_link = []\n",
    "\n",
    "    for image in img.findAll('img', attrs={'srcset':True}):\n",
    "        image = image.attrs['srcset'][1+image.attrs['srcset'].find(','):image.attrs['srcset'].find('.310w')-4]\n",
    "        if ('data:image') in image:\n",
    "                pass\n",
    "        else:\n",
    "            urllib.request.urlretrieve(image, product_dir+'/'+data_dir+str(counter)+'.jpg')\n",
    "            image_link.append(image)\n",
    "            counter += 1\n",
    "    \n",
    "    for website in img.findAll('a', {'href':True}):\n",
    "        website = website['href']\n",
    "        website_link.append(website)\n",
    "\n",
    "    if i == 1:\n",
    "        for price in img.findAll('div',{'class':'ProductCard-pricing'}):\n",
    "            price = price.find('span', {'class':'ProductCard-price'}).text\n",
    "            prices.append(price)    \n",
    "    else:\n",
    "        for price in img.findAll('div',{'class':'BrowseMinimizedPriceBlock'}):\n",
    "            price = price.find('span', {'class':'BrowseMinimizedPriceBlock-price'}).text\n",
    "            prices.append(price)\n",
    "\n",
    "\n",
    "    data = pd.DataFrame(dict(zip(['website_link', 'image_link', 'prices'],[website_link, image_link, prices])))\n",
    "    data['category'] = data_dir\n",
    "    full_df_lamp = full_df_lamp.append(data)\n",
    "    \n",
    "    if i == 5:\n",
    "        break\n",
    "    element = driver.find_element_by_xpath('//*[@id=\"sbprodgrid\"]/div[3]/div/nav/a[{}]'.format(i))\n",
    "    element.click()\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_df_lamp = pd.DataFrame(columns=['website_link', 'image_link', 'prices', 'category'])\n",
    "\n",
    "data_dir = 'lamp'\n",
    "\n",
    "# product_dir = os.path.join('images', data_dir)\n",
    "# counter = 0\n",
    "# if not os.path.exists(product_dir):\n",
    "#     os.makedirs(product_dir)\n",
    "\n",
    "path = \"/usr/local/bin/chromedriver\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "url = 'https://www.allmodern.com/lighting/sb0/floor-lamps-c477075.html'\n",
    "driver.get(url)\n",
    "\n",
    "for i in range(1, 4):\n",
    "    time.sleep(2)\n",
    "    #driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    if  4 > i >= 2:\n",
    "        i = i+1\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    img = soup.find('div', {'data-hb-id': 'pl-grid-v2'})\n",
    "\n",
    "    image_link = []\n",
    "    prices = []\n",
    "    website_link = []\n",
    "\n",
    "    for image in img.findAll('img', attrs={'srcset':True}):\n",
    "        image = image.attrs['srcset'][1+image.attrs['srcset'].find(','):image.attrs['srcset'].find('.310w')-4]\n",
    "        if ('data:image') in image:\n",
    "                pass\n",
    "        else:\n",
    "            urllib.request.urlretrieve(image, product_dir+'/'+data_dir+str(counter)+'.jpg')\n",
    "            image_link.append(image)\n",
    "            counter += 1\n",
    "    \n",
    "    for website in img.findAll('a', {'href':True}):\n",
    "        website = website['href']\n",
    "        website_link.append(website)\n",
    "\n",
    "    if i == 1:\n",
    "        for price in img.findAll('div',{'class':'ProductCard-pricing'}):\n",
    "            price = price.find('span', {'class':'ProductCard-price'}).text\n",
    "            prices.append(price)    \n",
    "    else:\n",
    "        for price in img.findAll('div',{'class':'BrowseMinimizedPriceBlock'}):\n",
    "            price = price.find('span', {'class':'BrowseMinimizedPriceBlock-price'}).text\n",
    "            prices.append(price)\n",
    "\n",
    "\n",
    "    data = pd.DataFrame(dict(zip(['website_link', 'image_link', 'prices'],[website_link, image_link, prices])))\n",
    "    data['category'] = data_dir\n",
    "    full_df_lamp = full_df_lamp.append(data)\n",
    "    \n",
    "    if i == 4:\n",
    "        break\n",
    "    element = driver.find_element_by_xpath('//*[@id=\"sbprodgrid\"]/div[3]/div/nav/a[{}]'.format(i))\n",
    "    element.click()\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dining Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_diningtable = pd.DataFrame(columns=['website_link', 'image_link', 'prices', 'category'])\n",
    "\n",
    "data_dir = 'dining tables'\n",
    "\n",
    "product_dir = os.path.join('images', data_dir)\n",
    "counter = 0\n",
    "if not os.path.exists(product_dir):\n",
    "    os.makedirs(product_dir)\n",
    "\n",
    "path = \"/usr/local/bin/chromedriver\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "url = 'https://www.allmodern.com/furniture/sb0/dining-tables-c366120.html'\n",
    "driver.get(url)\n",
    "\n",
    "counter = 0 \n",
    "\n",
    "for i in range(1, 9):\n",
    "    time.sleep(2)\n",
    "    #driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    if  5 > i >= 2:\n",
    "        i = i+1\n",
    "        \n",
    "    elif 7 > i >= 5:\n",
    "        i = 4\n",
    "    \n",
    "    elif i == 7:\n",
    "        i = 5\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    img = soup.find('div', {'data-hb-id': 'pl-grid-v2'})\n",
    "\n",
    "    image_link = []\n",
    "    prices = []\n",
    "    website_link = []\n",
    "\n",
    "    for image in img.findAll('img', attrs={'srcset':True}):\n",
    "        image = image.attrs['srcset'][1+image.attrs['srcset'].find(','):image.attrs['srcset'].find('.310w')-4]\n",
    "        if ('data:image') in image:\n",
    "                pass\n",
    "        else:\n",
    "            urllib.request.urlretrieve(image, product_dir+'/'+data_dir+str(counter)+'.jpg')\n",
    "            image_link.append(image)\n",
    "            counter += 1\n",
    "    \n",
    "    for website in img.findAll('a', {'href':True}):\n",
    "        website = website['href']\n",
    "        website_link.append(website)\n",
    "\n",
    "    if i == 1:\n",
    "        for price in img.findAll('div',{'class':'ProductCard-pricing'}):\n",
    "            price = price.find('span', {'class':'ProductCard-price'}).text\n",
    "            prices.append(price)    \n",
    "    else:\n",
    "        for price in img.findAll('div',{'class':'BrowseMinimizedPriceBlock'}):\n",
    "            price = price.find('span', {'class':'BrowseMinimizedPriceBlock-price'}).text\n",
    "            prices.append(price)\n",
    "\n",
    "\n",
    "    data = pd.DataFrame(dict(zip(['website_link', 'image_link', 'prices'],[website_link, image_link, prices])))\n",
    "    data['category'] = data_dir\n",
    "    full_df_diningtable = full_df_diningtable.append(data)\n",
    "    \n",
    "    if i == 8:\n",
    "        break\n",
    "    element = driver.find_element_by_xpath('//*[@id=\"sbprodgrid\"]/div[3]/div/nav/a[{}]'.format(i))\n",
    "    element.click()\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_df_diningtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dining chair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_diningchair = pd.DataFrame(columns=['website_link', 'image_link', 'prices', 'category'])\n",
    "\n",
    "data_dir = 'dining chair'\n",
    "\n",
    "product_dir = os.path.join('images', data_dir)\n",
    "counter = 0\n",
    "if not os.path.exists(product_dir):\n",
    "    os.makedirs(product_dir)\n",
    "\n",
    "path = \"/usr/local/bin/chromedriver\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "url = 'https://www.allmodern.com/furniture/sb0/dining-chairs-c366121.html'\n",
    "driver.get(url)\n",
    "\n",
    "counter = 0 \n",
    "\n",
    "for i in range(1, 7):\n",
    "    time.sleep(2)\n",
    "    #driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    if  i >= 2:\n",
    "        i = i+1\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    img = soup.find('div', {'data-hb-id': 'pl-grid-v2'})\n",
    "\n",
    "    image_link = []\n",
    "    prices = []\n",
    "    website_link = []\n",
    "\n",
    "    for image in img.findAll('img', attrs={'srcset':True}):\n",
    "        image = image.attrs['srcset'][1+image.attrs['srcset'].find(','):image.attrs['srcset'].find('.310w')-4]\n",
    "        if ('data:image') in image:\n",
    "                pass\n",
    "        else:\n",
    "            urllib.request.urlretrieve(image, product_dir+'/'+data_dir+str(counter)+'.jpg')\n",
    "            image_link.append(image)\n",
    "            counter += 1\n",
    "    \n",
    "    for website in img.findAll('a', {'href':True}):\n",
    "        website = website['href']\n",
    "        website_link.append(website)\n",
    "\n",
    "\n",
    "    if i == 1:\n",
    "        for price in img.findAll('div',{'class':'ProductCard-pricing'}):\n",
    "            price = price.find('span', {'class':'ProductCard-price'}).text\n",
    "            prices.append(price)    \n",
    "    else:\n",
    "        for price in img.findAll('div',{'class':'BrowseMinimizedPriceBlock'}):\n",
    "            price = price.find('span', {'class':'BrowseMinimizedPriceBlock-price'}).text\n",
    "            prices.append(price)\n",
    "\n",
    "\n",
    "    data = pd.DataFrame(dict(zip(['website_link', 'image_link', 'prices'],[website_link, image_link, prices])))\n",
    "    data['category'] = data_dir\n",
    "    full_df_diningchair = full_df_diningchair.append(data)\n",
    "    \n",
    "    if i == 7:\n",
    "        break\n",
    "    element = driver.find_element_by_xpath('//*[@id=\"sbprodgrid\"]/div[3]/div/nav/a[{}]'.format(i))\n",
    "    element.click()\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = full_df_diningchair.append(full_df_diningtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        $98\n",
       "1       $130\n",
       "2        $73\n",
       "3       $320\n",
       "4       $181\n",
       "       ...  \n",
       "6     $1,820\n",
       "7       $900\n",
       "8     $1,449\n",
       "9       $980\n",
       "10    $1,000\n",
       "Name: prices, Length: 612, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['prices'].apply(lambda x: x.split()[1] if len(x.split()) > 1 else x.split()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_rug = pd.DataFrame(columns=['website_link', 'image_link', 'prices', 'category'])\n",
    "\n",
    "data_dir = 'rug'\n",
    "\n",
    "product_dir = os.path.join('images', data_dir)\n",
    "counter = 0\n",
    "if not os.path.exists(product_dir):\n",
    "    os.makedirs(product_dir)\n",
    "\n",
    "path = \"/usr/local/bin/chromedriver\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "url = 'https://www.allmodern.com/rugs/sb0/area-rugs-c409269.html'\n",
    "driver.get(url)\n",
    "\n",
    "counter = 0 \n",
    "\n",
    "for i in range(1, 13):\n",
    "    time.sleep(2)\n",
    "    #driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    if  5 > i >= 2:\n",
    "        i = i+1\n",
    "        \n",
    "    elif 11 > i >= 5:\n",
    "        i = 4\n",
    "\n",
    "    elif i == 11:\n",
    "        i = 5\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    img = soup.find('div', {'data-hb-id': 'pl-grid-v2'})\n",
    "\n",
    "    image_link = []\n",
    "    prices = []\n",
    "    website_link = []\n",
    "\n",
    "    for image in img.findAll('img', attrs={'srcset':True}):\n",
    "        image = image.attrs['srcset'][1+image.attrs['srcset'].find(','):image.attrs['srcset'].find('.310w')-4]\n",
    "        if ('data:image') in image:\n",
    "                pass\n",
    "        else:\n",
    "            urllib.request.urlretrieve(image, product_dir+'/'+data_dir+str(counter)+'.jpg')\n",
    "            image_link.append(image)\n",
    "            counter += 1\n",
    "    \n",
    "    for website in img.findAll('a', {'href':True}):\n",
    "        website = website['href']\n",
    "        website_link.append(website)\n",
    "\n",
    "    if i == 1:\n",
    "        for price in img.findAll('div',{'class':'ProductCard-pricing'}):\n",
    "            price = price.find('span', {'class':'ProductCard-price'}).text\n",
    "            prices.append(price)    \n",
    "    else:\n",
    "        for price in img.findAll('div',{'class':'BrowseMinimizedPriceBlock'}):\n",
    "            price = price.find('span', {'class':'BrowseMinimizedPriceBlock-price'}).text\n",
    "            prices.append(price)\n",
    "\n",
    "\n",
    "    data = pd.DataFrame(dict(zip(['website_link', 'image_link', 'prices'],[website_link, image_link, prices])))\n",
    "    data['category'] = data_dir\n",
    "    full_df_rug = full_df_rug.append(data)\n",
    "\n",
    "    if i == 12:\n",
    "        break\n",
    "\n",
    "    element = driver.find_element_by_xpath('//*[@id=\"sbprodgrid\"]/div[3]/div/nav/a[{}]'.format(i))\n",
    "    element.click()\n",
    "    \n",
    "    time.sleep(10)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wall Art\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_wallart = pd.DataFrame(columns=['website_link', 'image_link', 'prices', 'category'])\n",
    "\n",
    "data_dir = 'wall art'\n",
    "\n",
    "product_dir = os.path.join('images', data_dir)\n",
    "counter = 0\n",
    "if not os.path.exists(product_dir):\n",
    "    os.makedirs(product_dir)\n",
    "\n",
    "path = \"/usr/local/bin/chromedriver\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "url = 'https://www.allmodern.com/decor-pillows/sb0/wall-art-c446716.html?prefetch=true'\n",
    "driver.get(url)\n",
    "\n",
    "counter = 0 \n",
    "\n",
    "for i in range(1, 16):\n",
    "    time.sleep(2)\n",
    "    #driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    if  5 > i >= 2:\n",
    "        i = i+1\n",
    "        \n",
    "    elif 14 > i >= 5:\n",
    "        i = 4\n",
    "\n",
    "    elif i == 14:\n",
    "        i = 5\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    img = soup.find('div', {'data-hb-id': 'pl-grid-v2'})\n",
    "\n",
    "    image_link = []\n",
    "    prices = []\n",
    "    website_link = []\n",
    "\n",
    "    for image in img.findAll('img', attrs={'srcset':True}):\n",
    "        image = image.attrs['srcset'][1+image.attrs['srcset'].find(','):image.attrs['srcset'].find('.310w')-4]\n",
    "        if ('data:image') in image:\n",
    "                pass\n",
    "        else:\n",
    "            urllib.request.urlretrieve(image, product_dir+'/'+data_dir+str(counter)+'.jpg')\n",
    "            image_link.append(image)\n",
    "            counter += 1\n",
    "    \n",
    "    for website in img.findAll('a', {'href':True}):\n",
    "        website = website['href']\n",
    "        website_link.append(website)\n",
    "\n",
    "    # if i == 1:\n",
    "    for price in img.findAll('div',{'class':'ProductCard-pricing'}):\n",
    "        price = price.find('span', {'class':'ProductCard-price'}).text\n",
    "        prices.append(price)    \n",
    "    # else:\n",
    "    #     for price in img.findAll('div',{'class':'BrowseMinimizedPriceBlock'}):\n",
    "    #         price = price.find('span', {'class':'BrowseMinimizedPriceBlock-price'}).text\n",
    "    #         prices.append(price)\n",
    "\n",
    "\n",
    "    data = pd.DataFrame(dict(zip(['website_link', 'image_link', 'prices'],[website_link, image_link, prices])))\n",
    "    data['category'] = data_dir\n",
    "    full_df_wallart = full_df_wallart.append(data)\n",
    "\n",
    "    if i == 15:\n",
    "        break\n",
    "\n",
    "    element = driver.find_element_by_xpath('//*[@id=\"sbprodgrid\"]/div[3]/div/nav/a[{}]'.format(i))\n",
    "    element.click()\n",
    "    \n",
    "    time.sleep(10)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = [full_df_rug, full_df_lamp, full_df_diningchair, full_df_diningtable, full_df_sofa, full_df_chair, full_df_coffeetable, full_df_consoletable, full_df_endtable, full_df_ottomans, full_df_tvstand]\n",
    "\n",
    "full_df = pd.DataFrame(columns=['website_link', 'image_link', 'prices', 'category'])\n",
    "\n",
    "for i in list:\n",
    "    full_df = full_df.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle.load(open('full_df.p', 'rb'))\n",
    "\n",
    "grouped = df.groupby('category', as_index=False)\n",
    "grouped_index = grouped.apply(lambda x: x.reset_index(drop = True)).reset_index()\n",
    "result = grouped_index.drop('level_0',axis = 1).set_index('level_1').reset_index()\n",
    "result['id'] = result.apply(lambda x: x['category']+str(x['level_1']), axis=1)\n",
    "\n",
    "pickle.dump(result, open('result.p', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
